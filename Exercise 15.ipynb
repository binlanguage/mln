{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d9fdeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T05:10:24.070977Z",
     "start_time": "2024-04-22T05:10:04.242274Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77df2b1",
   "metadata": {},
   "source": [
    "## SketchRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd359833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T00:36:50.218394Z",
     "start_time": "2024-04-15T00:34:19.670722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://download.tensorflow.org/data/quickdraw_tutorial_dataset_v1.tar.gz\n",
      "1065301781/1065301781 [==============================] - 132s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DOWNLOAD_ROOT = \"http://download.tensorflow.org/data/\"\n",
    "FILENAME = \"quickdraw_tutorial_dataset_v1.tar.gz\"\n",
    "filepath = keras.utils.get_file(FILENAME,\n",
    "                                DOWNLOAD_ROOT + FILENAME,\n",
    "                                cache_subdir=\"datasets/quickdraw\",\n",
    "                                extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea58ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T01:55:21.244148Z",
     "start_time": "2024-04-11T01:55:21.238147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets/quickdraw\\\\quickdraw_tutorial_dataset_v1.tar.gz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11d4a12e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T01:09:38.371425Z",
     "start_time": "2024-04-12T01:09:37.917921Z"
    }
   },
   "outputs": [],
   "source": [
    "quickdraw_dir = Path(filepath).parent\n",
    "all_files= sorted([str(file) for file in quickdraw_dir.glob(\"*.*\")])\n",
    "train_files = sorted([str(path) for path in quickdraw_dir.glob(\"training.tfrecord-*\")])\n",
    "eval_files = sorted([str(path) for path in quickdraw_dir.glob(\"eval.tfrecord-*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc347f5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T01:09:39.181942Z",
     "start_time": "2024-04-12T01:09:39.137934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00000-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00001-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00002-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00003-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00004-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00005-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00006-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00007-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00008-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00009-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord.classes',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00000-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00001-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00002-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00003-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00004-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00005-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00006-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00007-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00008-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord-00009-of-00010',\n",
       " 'C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\training.tfrecord.classes']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a966f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:11:39.118218Z",
     "start_time": "2024-04-11T07:11:39.110217Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#读class 文件\n",
    "with open(quickdraw_dir / \"eval.tfrecord.classes\") as test_classes_file:\n",
    "    test_classes = test_classes_file.readlines()\n",
    "    \n",
    "with open(quickdraw_dir / \"training.tfrecord.classes\") as train_classes_file:\n",
    "    train_classes = train_classes_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2f5404ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T01:12:38.434313Z",
     "start_time": "2024-04-12T01:12:38.110051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert test_classes==train_classes #验证测试类别标签和训练标签类别一致\n",
    "class_names = [name.strip().lower() for name in train_classes] #去除类别标签前后空格，转小写\n",
    "len(class_names) #345个分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a4415b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:57:49.789127Z",
     "start_time": "2024-04-11T07:57:49.785129Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse(data_batch):\n",
    "    feature_descriptions = {\n",
    "        \"ink\": tf.io.VarLenFeature(dtype=tf.float32),#变长特征\n",
    "        \"shape\": tf.io.FixedLenFeature([2], dtype=tf.int64),#定长特征\n",
    "        \"class_index\": tf.io.FixedLenFeature([1], dtype=tf.int64)\n",
    "    }\n",
    "    examples = tf.io.parse_example(data_batch, feature_descriptions)\n",
    "    flat_sketches = tf.sparse.to_dense(examples[\"ink\"])\n",
    "    sketches = tf.reshape(flat_sketches, shape=[tf.size(data_batch), -1, 3])\n",
    "    lengths = examples[\"shape\"][:, 0]\n",
    "    labels = examples[\"class_index\"][:, 0]\n",
    "    return sketches, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9cb2556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:57:50.454714Z",
     "start_time": "2024-04-11T07:57:50.449710Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def quickdraw_dataset(filepaths, batch_size=32, shuffle_buffer_size=None,\n",
    "                      n_parse_threads=5, n_read_threads=5, cache=False):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths,\n",
    "                                      num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(parse, num_parallel_calls=n_parse_threads)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56cdc1ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T01:10:24.536185Z",
     "start_time": "2024-04-12T01:10:13.016122Z"
    }
   },
   "outputs": [],
   "source": [
    "examplefile='C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\quickdraw\\\\eval.tfrecord-00004-of-00010'\n",
    "dataset = tf.data.TFRecordDataset(examplefile)\n",
    "bytedataset=list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "270bd882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T01:10:50.876263Z",
     "start_time": "2024-04-12T01:10:48.179596Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_descriptions = {\n",
    "        \"ink\": tf.io.VarLenFeature(dtype=tf.float32),#变长特征\n",
    "        \"shape\": tf.io.FixedLenFeature([2], dtype=tf.int64),#定长特征\n",
    "        \"class_index\": tf.io.FixedLenFeature([1], dtype=tf.int64)\n",
    "    }\n",
    "dataset_example = tf.io.parse_example(bytedataset, feature_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "27103b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T01:11:10.413341Z",
     "start_time": "2024-04-12T01:11:10.289331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(34279, 2040), dtype=float32, numpy=\n",
       "array([[-0.0863, -0.0354,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       [-0.0472, -0.019 ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       [-0.0748, -0.0442,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       ...,\n",
       "       [ 0.1391,  0.0157,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.0235, -0.0714,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.0902, -0.0127,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(dataset_example[\"ink\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475715e",
   "metadata": {},
   "source": [
    "## jsb_chorales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e90b67ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T05:10:25.930390Z",
     "start_time": "2024-04-22T05:10:24.070977Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chorales_file = tf.keras.utils.get_file('jsb_chorales', \n",
    "                                       'https://raw.githubusercontent.com/ageron/data/main/jsb_chorales.tgz',\n",
    "                                        untar=True)\n",
    "\n",
    "chorales_root = Path(chorales_file)\n",
    "\n",
    "train_chorales_list=sorted([str(file) for file in chorales_root.glob(\"train/*.csv\")])\n",
    "valid_chorales_list=sorted([str(file) for file in chorales_root.glob(\"valid/*.csv\")])\n",
    "test_chorales_list=sorted([str(file) for file in chorales_root.glob(\"test/*.csv\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e692b58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:07:09.351276Z",
     "start_time": "2024-04-16T07:07:09.335653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228 entries, 0 to 227\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   note0   228 non-null    int64\n",
      " 1   note1   228 non-null    int64\n",
      " 2   note2   228 non-null    int64\n",
      " 3   note3   228 non-null    int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 7.2 KB\n"
     ]
    }
   ],
   "source": [
    "#看下基本数据结构，四个int64列\n",
    "csv_example='C:\\\\Users\\\\sec_zca9cu_rw\\\\.keras\\\\datasets\\\\jsb_chorales\\\\test\\\\chorale_305.csv'\n",
    "\n",
    "df = pd.read_csv(csv_example)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2646bcb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T05:10:38.790041Z",
     "start_time": "2024-04-22T05:10:25.930390Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_chorales_list)\n",
    "valid_chorales = load_chorales(valid_chorales_list)\n",
    "test_chorales = load_chorales(test_chorales_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e025d4e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T05:10:38.821291Z",
     "start_time": "2024-04-22T05:10:38.790041Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "notes = set() #空集\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord) #并集\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0}) #去除0元素，即休止符\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36 #C1 = C on octave 1 第一个八度的C\n",
    "assert max_note == 81 #A5 = A on octave 5 第五个八度的A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c905578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T05:10:38.899419Z",
     "start_time": "2024-04-22T05:10:38.821291Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "def notes_to_frequencies(notes):\n",
    "    # Frequency doubles when you go up one octave; there are 12 semi-tones\n",
    "    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n",
    "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
    "\n",
    "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
    "    note_duration = 60 / tempo # the tempo is measured in beats per minutes 一个音持续多少秒\n",
    "    # To reduce click sound at every beat, we round the frequencies to try to\n",
    "    # get the samples close to zero at the end of each note.\n",
    "    frequencies = np.round(note_duration * frequencies) / note_duration #对频率*时长取整\n",
    "    n_samples = int(note_duration * sample_rate) #一个音符需要采样多少个音频信号\n",
    "    time = np.linspace(0, note_duration, n_samples)#一个音对采样频率等分时间\n",
    "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time) #frequencies.reshape(-1, 1)为array([[frequencies]])\n",
    "    # Removing all notes with frequencies ≤ 9 Hz (includes note 0 = silence)\n",
    "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
    "    return sine_waves.reshape(-1)#其实不太能get这个reshape到(-1,1)乘布尔值再reshape(-1)的操作\n",
    "\n",
    "def chords_to_samples(chords, tempo, sample_rate):\n",
    "    freqs = notes_to_frequencies(chords)\n",
    "    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer 把频率列表最后一个值重复一遍\n",
    "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
    "                     for melody in freqs.T], axis=0) #把每个声部的频率转换为信号，再取平均，等同于傅里叶级数\n",
    "    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note 向下取整除法，获取最后一个音的采样数\n",
    "    fade_out = np.linspace(1., 0., n_fade_out_samples)**2 \n",
    "    merged[-n_fade_out_samples:] *= fade_out\n",
    "    return merged\n",
    "\n",
    "def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n",
    "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
    "    if filepath:\n",
    "        from scipy.io import wavfile\n",
    "        samples = (2**15 * samples).astype(np.int16)\n",
    "        wavfile.write(filepath, sample_rate, samples)\n",
    "        return display(Audio(filepath))\n",
    "    else:\n",
    "        return display(Audio(samples, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "90b0323e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T09:28:03.959677Z",
     "start_time": "2024-04-22T09:28:03.819061Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tempo=160\n",
    "sample_rate=44100\n",
    "\n",
    "freqs=notes_to_frequencies(train_chorales[np.random.randint(0,len(train_chorales_list))])\n",
    "freqclip=freqs[0:np.random.randint(0,len(freqs))]\n",
    "\n",
    "merged= np.mean([frequencies_to_samples(melody, tempo=tempo, sample_rate=sample_rate) for melody in freqclip.T],axis=0)\n",
    "waveclip=frequencies_to_samples(freqclip, tempo=tempo, sample_rate=sample_rate)\n",
    "\n",
    "display(Audio(merged, rate=sample_rate)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc266f09",
   "metadata": {},
   "source": [
    "In order to be able to generate new chorales, we want to train a model that can predict the next chord given all the previous chords. If we naively try to predict the next chord in one shot, predicting all 4 notes at once, we run the risk of getting notes that don't go very well together (believe me, I tried). It's much better and simpler to predict one note at a time. So we will need to preprocess every chorale, turning each chord into an arpegio 分解和弦 (i.e., a sequence of notes rather than notes played simultaneuously). So each chorale will be a long sequence of notes (rather than chords), and we can just train a model that can predict the next note given all the previous notes. We will use a sequence-to-sequence approach, where we feed a window to the neural net, and it tries to predict that same window shifted one time step into the future.\n",
    "\n",
    "We will also shift the values so that they range from 0 to 46, where 0 represents silence, and values 1 to 46 represent notes 36 (C1) to 81 (A5).\n",
    "\n",
    "And we will train the model on windows of 128 notes (i.e., 32 chords).\n",
    "\n",
    "Since the dataset fits in memory, we could preprocess the chorales in RAM using any Python code we like, but I will demonstrate here how to do all the preprocessing using tf.data (there will be more details about creating windows using tf.data in the next chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac5c19",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def create_target(batch):\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n",
    "    return tf.reshape(window, [-1]) # convert to arpegio\n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(create_target)\n",
    "    return dataset.prefetch(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
