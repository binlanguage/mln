{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfb0893",
   "metadata": {},
   "source": [
    "# Stack Overflow problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f51acc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T00:55:51.300444Z",
     "start_time": "2024-05-11T00:55:10.360616Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "import pathlib\n",
    "\n",
    "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
    "\n",
    "dataset_dir = utils.get_file(\n",
    "    origin=data_url,\n",
    "    untar=True,\n",
    "    cache_dir='stack_overflow',\n",
    "    cache_subdir='')\n",
    "\n",
    "dataset_dir = pathlib.Path(dataset_dir).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0366a2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T00:55:51.316067Z",
     "start_time": "2024-05-11T00:55:51.300444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/tmp/.keras/README.md'),\n",
       " PosixPath('/tmp/.keras/stack_overflow_16k.tar.gz'),\n",
       " PosixPath('/tmp/.keras/test'),\n",
       " PosixPath('/tmp/.keras/train')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#注意文件夹，只有train和test，valid数据集需从train数据集中分离\n",
    "train_dir = dataset_dir/'train'\n",
    "test_dir=dataset_dir/'test'\n",
    "list(dataset_dir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668943e-88ae-44b8-8960-2a779c43f6f5",
   "metadata": {},
   "source": [
    "## CNN+dense 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81daa175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T00:55:52.427645Z",
     "start_time": "2024-05-11T00:55:51.317469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n",
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# text_dataset_from_directory +TextVectorization\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "#拆分训练集和验证集\n",
    "#保持seed一致，或者都shuffle=false\n",
    "\n",
    "raw_train_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "raw_val_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "raw_test_ds = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03a0d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:39.169260Z",
     "start_time": "2024-05-07T08:30:39.153640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to csharp\n",
      "Label 1 corresponds to java\n",
      "Label 2 corresponds to javascript\n",
      "Label 3 corresponds to python\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(raw_train_ds.class_names):\n",
    "    print(\"Label\", i, \"corresponds to\", label)\n",
    "    \n",
    "num_labels=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575c5e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:39.341135Z",
     "start_time": "2024-05-07T08:30:39.169260Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#对数据进行标准化、词例化和向量化\n",
    "#文本转换为小写，按空格分割，向量化模式为int\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "VOCAB_SIZE =  10000 #词汇量\n",
    "MAX_SEQUENCE_LENGTH = 250 #最大序列长度\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37508757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:43.724109Z",
     "start_time": "2024-05-07T08:30:39.341135Z"
    }
   },
   "outputs": [],
   "source": [
    "#调用 TextVectorization.adapt 以使预处理层的状态适合数据集。这会使模型构建字符串到整数的索引。\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "int_vectorize_layer.adapt(train_text)\n",
    "\n",
    "#Once the vocabulary is set, the layer can encode text into indices. \n",
    "#The tensors of indices are 0-padded to the longest sequence in the batch \n",
    "#unless you set a fixed output_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f92002d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:43.849108Z",
     "start_time": "2024-05-07T08:30:43.724109Z"
    }
   },
   "outputs": [],
   "source": [
    "#创建数据集\n",
    "import tensorflow as tf\n",
    "\n",
    "def int_vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return int_vectorize_layer(text), label\n",
    "\n",
    "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
    "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
    "int_test_ds = raw_test_ds.map(int_vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8274bf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:43.911607Z",
     "start_time": "2024-05-07T08:30:43.849108Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras  \n",
    "\n",
    "model = keras.Sequential([\n",
    "      keras.layers.Embedding(VOCAB_SIZE, 64, mask_zero=True),\n",
    "      keras.layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      keras.layers.GlobalMaxPooling1D(),\n",
    "      keras.layers.Dense(num_labels,activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c818c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:31:30.929477Z",
     "start_time": "2024-05-07T08:30:43.911607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 10s 46ms/step - loss: 1.1578 - accuracy: 0.5089 - val_loss: 0.7892 - val_accuracy: 0.6600\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 0.6247 - accuracy: 0.7595 - val_loss: 0.5426 - val_accuracy: 0.7937\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.3739 - accuracy: 0.8834 - val_loss: 0.4775 - val_accuracy: 0.8163\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.2077 - accuracy: 0.9505 - val_loss: 0.4763 - val_accuracy: 0.8206\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.1038 - accuracy: 0.9822 - val_loss: 0.4948 - val_accuracy: 0.8200\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "history = model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e819087f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:42:43.572649Z",
     "start_time": "2024-05-07T08:42:37.424070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 6s 23ms/step - loss: 0.5026 - accuracy: 0.8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5026109218597412, 0.8132500052452087]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(int_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1f5a8",
   "metadata": {},
   "source": [
    "## TFHUB 训练方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a5da21c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T01:52:53.811419Z",
     "start_time": "2024-05-11T01:52:51.998901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 18:48:57.610463: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:48:58.646811: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:48:58.647595: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:48:58.661400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:48:58.662211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:48:58.662886: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:49:02.947956: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:49:02.948621: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:49:02.948636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-11 18:49:02.949220: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 18:49:02.949240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-05-11 18:49:04.500250: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n",
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf  \n",
    "from tensorflow.keras import utils\n",
    "seed=42\n",
    "#如果用tfhub训练文本，则不能给数据集打批次\n",
    "\n",
    "hub_train_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=None,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "hub_val_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=None,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "hub_test_ds = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=None)\n",
    "\n",
    "\n",
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f92227e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T01:52:53.827048Z",
     "start_time": "2024-05-11T01:52:53.811419Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "hub_train_ds = hub_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "hub_val_ds = hub_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "hub_test_ds = hub_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accd3fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T05:21:50.060650Z",
     "start_time": "2024-05-11T05:21:49.951279Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 18:50:01.996578: I external/local_xla/xla/service/service.cc:168] XLA service 0x559a43956c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-11 18:50:01.996619: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti, Compute Capability 8.6\n",
      "2024-05-11 18:50:02.126871: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-11 18:50:03.273380: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715424603.624034     204 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 30s 96ms/step - loss: 0.8420 - accuracy: 0.6670 - val_loss: 0.4822 - val_accuracy: 0.8244\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 14s 69ms/step - loss: 0.3513 - accuracy: 0.8803 - val_loss: 0.4057 - val_accuracy: 0.8537\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 0.1678 - accuracy: 0.9484 - val_loss: 0.4349 - val_accuracy: 0.8487\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 0.0828 - accuracy: 0.9781 - val_loss: 0.4713 - val_accuracy: 0.8587\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 9s 44ms/step - loss: 0.0432 - accuracy: 0.9900 - val_loss: 0.5341 - val_accuracy: 0.8481\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 8s 37ms/step - loss: 0.0225 - accuracy: 0.9953 - val_loss: 0.5887 - val_accuracy: 0.8475\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.0126 - accuracy: 0.9986 - val_loss: 0.6367 - val_accuracy: 0.8481\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0087 - accuracy: 0.9989 - val_loss: 0.6792 - val_accuracy: 0.8469\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 0.7114 - val_accuracy: 0.8475\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 6s 30ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.7429 - val_accuracy: 0.8456\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.0113 - accuracy: 0.9986 - val_loss: 0.8224 - val_accuracy: 0.8487\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.0071 - accuracy: 0.9989 - val_loss: 0.8680 - val_accuracy: 0.8313\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "modelhub = keras.Sequential()\n",
    "modelhub.add(hub_layer)\n",
    "modelhub.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "modelhub.add(tf.keras.layers.Dense(64))\n",
    "modelhub.add(keras.layers.Dropout(rate=0.2))\n",
    "modelhub.add(keras.layers.Dense(4,activation=\"softmax\"))\n",
    "\n",
    "modelhub.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "\n",
    "callbacks = [early_stopping_cb]\n",
    "\n",
    "#训练时候再打批次\n",
    "\n",
    "history = modelhub.fit(hub_train_ds.shuffle(10000).batch(32),\n",
    "                    epochs=100,\n",
    "                    validation_data=hub_val_ds.batch(128),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d728d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T03:09:04.740200Z",
     "start_time": "2024-05-11T03:09:03.146103Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 26ms/step - loss: 0.4229 - accuracy: 0.8394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42287933826446533, 0.8393750190734863]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#评估预测也要额外打批\n",
    "modelhub.evaluate(hub_test_ds.batch(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3ae3f35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T02:38:25.687333Z",
     "start_time": "2024-05-11T02:38:20.076058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(modelhub.predict(hub_test_ds.batch(512)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3ada4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
