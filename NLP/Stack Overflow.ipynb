{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfb0893",
   "metadata": {},
   "source": [
    "# Stack Overflow problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f51acc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T00:55:51.300444Z",
     "start_time": "2024-05-11T00:55:10.360616Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "import pathlib\n",
    "\n",
    "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
    "\n",
    "dataset_dir = utils.get_file(\n",
    "    origin=data_url,\n",
    "    untar=True,\n",
    "    cache_dir='stack_overflow',\n",
    "    cache_subdir='')\n",
    "\n",
    "dataset_dir = pathlib.Path(dataset_dir).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0366a2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T00:55:51.316067Z",
     "start_time": "2024-05-11T00:55:51.300444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('/tmp/.keras/README.md'),\n",
       " WindowsPath('/tmp/.keras/stack_overflow_16k.tar.gz'),\n",
       " WindowsPath('/tmp/.keras/test'),\n",
       " WindowsPath('/tmp/.keras/train')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#注意文件夹，只有train和test，valid数据集需从train数据集中分离\n",
    "train_dir = dataset_dir/'train'\n",
    "test_dir=dataset_dir/'test'\n",
    "list(dataset_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81daa175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T00:55:52.427645Z",
     "start_time": "2024-05-11T00:55:51.317469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n",
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# text_dataset_from_directory +TextVectorization\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "#拆分训练集和验证集\n",
    "#保持seed一致，或者都shuffle=false\n",
    "\n",
    "raw_train_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "raw_val_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "raw_test_ds = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03a0d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:39.169260Z",
     "start_time": "2024-05-07T08:30:39.153640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to csharp\n",
      "Label 1 corresponds to java\n",
      "Label 2 corresponds to javascript\n",
      "Label 3 corresponds to python\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(raw_train_ds.class_names):\n",
    "    print(\"Label\", i, \"corresponds to\", label)\n",
    "    \n",
    "num_labels=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575c5e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:39.341135Z",
     "start_time": "2024-05-07T08:30:39.169260Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#对数据进行标准化、词例化和向量化\n",
    "#文本转换为小写，按空格分割，向量化模式为int\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "VOCAB_SIZE =  10000 #词汇量\n",
    "MAX_SEQUENCE_LENGTH = 250 #最大序列长度\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37508757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:43.724109Z",
     "start_time": "2024-05-07T08:30:39.341135Z"
    }
   },
   "outputs": [],
   "source": [
    "#调用 TextVectorization.adapt 以使预处理层的状态适合数据集。这会使模型构建字符串到整数的索引。\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "int_vectorize_layer.adapt(train_text)\n",
    "\n",
    "#Once the vocabulary is set, the layer can encode text into indices. \n",
    "#The tensors of indices are 0-padded to the longest sequence in the batch \n",
    "#unless you set a fixed output_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f92002d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:43.849108Z",
     "start_time": "2024-05-07T08:30:43.724109Z"
    }
   },
   "outputs": [],
   "source": [
    "#创建数据集\n",
    "import tensorflow as tf\n",
    "\n",
    "def int_vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return int_vectorize_layer(text), label\n",
    "\n",
    "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
    "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
    "int_test_ds = raw_test_ds.map(int_vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8274bf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:30:43.911607Z",
     "start_time": "2024-05-07T08:30:43.849108Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras  \n",
    "\n",
    "model = keras.Sequential([\n",
    "      keras.layers.Embedding(VOCAB_SIZE, 64, mask_zero=True),\n",
    "      keras.layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      keras.layers.GlobalMaxPooling1D(),\n",
    "      keras.layers.Dense(num_labels,activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c818c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:31:30.929477Z",
     "start_time": "2024-05-07T08:30:43.911607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 10s 46ms/step - loss: 1.1578 - accuracy: 0.5089 - val_loss: 0.7892 - val_accuracy: 0.6600\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 0.6247 - accuracy: 0.7595 - val_loss: 0.5426 - val_accuracy: 0.7937\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.3739 - accuracy: 0.8834 - val_loss: 0.4775 - val_accuracy: 0.8163\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.2077 - accuracy: 0.9505 - val_loss: 0.4763 - val_accuracy: 0.8206\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.1038 - accuracy: 0.9822 - val_loss: 0.4948 - val_accuracy: 0.8200\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "history = model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e819087f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T08:42:43.572649Z",
     "start_time": "2024-05-07T08:42:37.424070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 6s 23ms/step - loss: 0.5026 - accuracy: 0.8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5026109218597412, 0.8132500052452087]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(int_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1f5a8",
   "metadata": {},
   "source": [
    "## TFHUB 训练方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a5da21c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T01:52:53.811419Z",
     "start_time": "2024-05-11T01:52:51.998901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n",
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf  \n",
    "\n",
    "#如果用tfhub训练文本，则不能给数据集打批次\n",
    "\n",
    "hub_train_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=None,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "hub_val_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=None,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "hub_test_ds = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=None)\n",
    "\n",
    "\n",
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f92227e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T01:52:53.827048Z",
     "start_time": "2024-05-11T01:52:53.811419Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "hub_train_ds = hub_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "hub_val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "hub_test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd3fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T05:21:50.060650Z",
     "start_time": "2024-05-11T05:21:49.951279Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "modelhub = keras.Sequential()\n",
    "modelhub.add(hub_layer)\n",
    "modelhub.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "modelhub.add(tf.keras.layers.Dense(16))\n",
    "modelhub.add(keras.layers.Dropout(rate=0.2))\n",
    "modelhub.add(keras.layers.Dense(4,activation=\"softmax\"))\n",
    "\n",
    "modelhub.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "\n",
    "callbacks = [early_stopping_cb]\n",
    "\n",
    "#训练时候再打批次\n",
    "\n",
    "history = modelhub.fit(hub_train_ds.shuffle(10000).batch(1024),\n",
    "                    epochs=100,\n",
    "                    validation_data=hub_val_ds.batch(512),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d728d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T03:09:04.740200Z",
     "start_time": "2024-05-11T03:09:03.146103Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 96ms/step - loss: 0.6700 - accuracy: 0.7486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6700264811515808, 0.7486249804496765]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#评估预测也要额外打批\n",
    "modelhub.evaluate(hub_test_ds.batch(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3ae3f35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T02:38:25.687333Z",
     "start_time": "2024-05-11T02:38:20.076058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 6s 88ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, ..., 2, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(modelhub.predict(hub_test_ds.batch(128)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3ada4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
